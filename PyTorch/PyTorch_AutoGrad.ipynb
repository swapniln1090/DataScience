{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This is Google Colab Notebook"
      ],
      "metadata": {
        "id": "IBAKpLMb4Y2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Gradient\n",
        "\n",
        "A gradient is essentially a vector (a direction and magnitude) that tells us how much and in which direction we need to change a parameter (like weights or biases in a neural network) to minimize or maximize a given function."
      ],
      "metadata": {
        "id": "oh4fbgIITwaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Consider we have an function say y = x^2. Now to calculate derivative of y w.r.t. x is dy/dx = 2x"
      ],
      "metadata": {
        "id": "CtZnyf0Kk7t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hence we can code this considering y = x**2\n",
        "\n",
        "def dy_dx(x):\n",
        "  return 2*x\n",
        "\n",
        "dy_dx(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqW51CpVmelq",
        "outputId": "376b9957-0790-4ff9-d469-0724114c6b2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now say we have 2 function where 1. y = x**2 and then 2. z = sin(y)\n",
        "# Now dz/dx = dz/dy * dy/dx\n",
        "\n",
        "import math\n",
        "\n",
        "def dy_dx(x):\n",
        "  return 2*x\n",
        "\n",
        "def dz_dx(x):\n",
        "  return dy_dx(x) * math.cos(x**2)\n",
        "\n",
        "dz_dx(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWK4DKBbmfgs",
        "outputId": "32b51246-e8ba-48a5-f180-0132f07d35e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-2.6145744834544478"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Now consider you have continously nested function like\n",
        "\n",
        " y = x**2\n",
        "\n",
        " z = sin(y)\n",
        "\n",
        " u = e**z\n",
        "\n",
        "Now here the derivative du/dx become more difficult to derive manually.\n",
        "\n",
        "In deep learning we have deep neural network which has very complex nested network/function. Then deriving the derivative through chain rule and keeping track of those derivative becomes very hard.\n",
        "\n",
        "Usually training process is -\n",
        "\n",
        "1. Forward pass\n",
        "\n",
        "2. Calculate Loss\n",
        "\n",
        "3. Backward Pass - Compute gradient of Loss w.r.t. parameters(weight and bias)\n",
        "\n",
        "4. Update Gradient - Update the parameters using optimizer algo(like Gradient Descent).\n",
        "\n",
        "\n",
        "Now to perform all this steps manually, it becomes very difficult and in deep neural network it can become nearly impossible.\n",
        "\n",
        "Hence, **PyTorch AutoGrad** simplifies the process for us.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vMWOLHa5nafS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What is Autograd?**\n",
        "Autograd is a system that performs automatic differentiation to compute the gradients of functions with respect to their inputs, parameters, or any variable. These gradients are crucial for optimizing the model during training (e.g in gradient descent).\n",
        "\n",
        "When you define a function in machine learning, Autograd keeps track of all operations performed on the inputs. Using this information, it calculates the derivatives (gradients) automatically without requiring you to manually derive formulas."
      ],
      "metadata": {
        "id": "n6iy7mUQpskR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Now taking similar examples as above, I am solving through PyTorch Autograd function"
      ],
      "metadata": {
        "id": "T8vnuyFErSV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "iGOy7Jn-rnx6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3, requires_grad= True) # whenever you need to find derivative of the tensor, then keep this parameter as True\n",
        "\n",
        "y = x**2\n",
        "\n",
        "# I am leaving this error deliberatly to know that this only works for floating point and complex dtypes and not for integer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ej6Jb-Kfrqaz",
        "outputId": "7bf7c619-f2dd-444e-bd8b-ea7a68e7d61b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Only Tensors of floating point and complex dtype can require gradients",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b1d1149c05f0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# whenever you need to find derivative of the tensor, then keep this parameter as True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad= True) # whenever you need to find derivative of the tensor, then keep this parameter as True\n",
        "\n",
        "y = x**2"
      ],
      "metadata": {
        "id": "C9hTZ-dBsHyZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2s9JugCsd-k",
        "outputId": "3e0c4843-fd1d-45a8-dee8-546826b37a53"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBRbYtjysfKo",
        "outputId": "b1b04dc3-d3ec-4f97-d0d5-39acba56a1fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(9., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here PyTorch internally creates a Computation Graph and stores the operations it has performed while Forward pass. Like in this case grad_fn=<PowBackward0>. Hence later at backward pass it can perform respective derivation.\n",
        "\n",
        "y.backward()"
      ],
      "metadata": {
        "id": "fLuVcg8wsf7-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqFK0fITszl0",
        "outputId": "30e3320b-210a-4065-8e74-5af71d61ca8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now taking 2nd example\n",
        "\n",
        "dz_dx(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtKpNXNYtK8p",
        "outputId": "2bc42368-a65e-499f-b47c-634d8a9b5d48"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.466781571308061"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try with AutoGrad functionality\n",
        "\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "y = x**2\n",
        "\n",
        "z = torch.sin(y)\n",
        "\n"
      ],
      "metadata": {
        "id": "58xpnu9HtWTM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y , z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2SuWVEpu693",
        "outputId": "cc13bed2-5ca1-4ee6-815c-fefb1c582cda"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(3., requires_grad=True),\n",
              " tensor(9., grad_fn=<PowBackward0>),\n",
              " tensor(0.4121, grad_fn=<SinBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "1AmUjiZ-u8r4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb4TQrcmvC-M",
        "outputId": "3265d498-afcf-41a9-f15f-95db38634d9a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-5.4668)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What if I try printing/fetching grad for y in this whole nested function.\n",
        "\n",
        "y.grad\n",
        "\n",
        "# We can't do that because when the computational graph is created, the gradients is not calculated with intermediate nodes. It is mainly calculated for Leaves Node."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAD_r3xAvFhZ",
        "outputId": "c9eec69e-77a1-4d84-9752-90d2af9aa552"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-cc4149527fa5>:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  y.grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
        "\n",
        "In a forward pass, autograd does two things simultaneously:\n",
        "\n",
        "\n",
        "*   run the requested operation to compute a resulting tensor\n",
        "*   maintain the operation’s gradient function in the DAG\n",
        "\n",
        "\n",
        "The backward pass kicks off when .backward() is called on the DAG root. autograd then:\n",
        "\n",
        "\n",
        "\n",
        "*   computes the gradients from each .grad_fn,\n",
        "*   accumulates them in the respective tensor’s .grad attribute\n",
        "*   using the chain rule, propagates all the way to the leaf tensors.\n",
        "\n",
        "\n",
        "\n",
        "*Reference* - https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#more-on-computational-graphs"
      ],
      "metadata": {
        "id": "lpX3-MSIvab6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### In Terms of Actual Perceptron\n",
        "\n",
        "1. Manual"
      ],
      "metadata": {
        "id": "pBj16Da7wtnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(6.7) # Input feature\n",
        "y = torch.tensor(0.0) # True Label (Binary)\n",
        "\n",
        "w = torch.tensor(1.0) # Weight\n",
        "b = torch.tensor(0.0) # Bias"
      ],
      "metadata": {
        "id": "uUOk22PgxZdU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary cross entropy loss\n",
        "\n",
        "def binary_cross_entropy_loss(pred, target):\n",
        "  epsilon = 1e-8   # to prevent log(0)\n",
        "  prediction = torch.clamp(pred, epsilon, 1 - epsilon)\n",
        "  return -(target * torch.log(prediction) + (1 - target) * torch.log(1 - prediction)) # Formula of Binary Loss function"
      ],
      "metadata": {
        "id": "S1u9HwCcx5NM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Pass\n",
        "\n",
        "z = w * x + b # Linear function\n",
        "y_pred = torch.sigmoid(z)\n",
        "\n",
        "# Compute Binary Loss\n",
        "loss = binary_cross_entropy_loss(y_pred, y)"
      ],
      "metadata": {
        "id": "MLjI6qJjyul_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZQdeCVtzUrT",
        "outputId": "3143fa0a-bd68-4b0d-d577-772cfd72ac26"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Back Propagation\n",
        "\n",
        "# Derivatives:\n",
        "# 1. dL/d(y_pred): Loss with respect to the prediction (y_pred)\n",
        "dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))\n",
        "\n",
        "# 2. dy_pred/dz: Prediction (y_pred) with respect to z (sigmoid derivative)\n",
        "dy_pred_dz = y_pred * (1 - y_pred)\n",
        "\n",
        "# 3. dz/dw and dz/db: z with respect to w and b\n",
        "dz_dw = x  # dz/dw = x\n",
        "dz_db = 1  # dz/db = 1 (bias contributes directly to z)\n",
        "\n",
        "dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw\n",
        "dL_db = dloss_dy_pred * dy_pred_dz * dz_db"
      ],
      "metadata": {
        "id": "3rJWo-2BzVu1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Manual Gradient of loss w.r.t weight (dw): {dL_dw}\")\n",
        "print(f\"Manual Gradient of loss w.r.t bias (db): {dL_db}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qshlic1zg22",
        "outputId": "155cae89-bb46-4cdf-9e29-a984517b57cf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Gradient of loss w.r.t weight (dw): 6.691762447357178\n",
            "Manual Gradient of loss w.r.t bias (db): 0.998770534992218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. AutoGrad"
      ],
      "metadata": {
        "id": "Memg90Sbzjns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)\n",
        "\n",
        "w = torch.tensor(1.0, requires_grad=True) # Because we need to calculate Gradient with res to these parameters\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "2KG9Ioc3zwRF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBJQkFN5z5wg",
        "outputId": "0cd0dd98-f2c7-46f2-b515-54d8b635a950"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(1., requires_grad=True), tensor(0., requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = w*x + b\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR5npD42z8dh",
        "outputId": "9f0c88af-1693-4cbf-e77a-f7f4e30f0191"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7000, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(z)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEj5JtFVz9qE",
        "outputId": "b5e6f78c-4d2b-47e0-c966-e214082d8ec5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjbJkMXC0Amw",
        "outputId": "9b964256-aa63-41ae-c6bb-0961e1f1a769"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.7012, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "yJb9pjCT0Cbx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN4aBph20GgY",
        "outputId": "6298fc8c-c432-420e-ef6a-97dc918cfd5a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.6918)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsFZ433F0KFt",
        "outputId": "a0ce4036-972d-4848-c92d-5652968a7961"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9988)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now trying with 1 dim array tensor\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMvAglod0Lav",
        "outputId": "12ca3282-87c9-4576-b1f2-6231b98ec06a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = (x**2).mean()\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBEkzoU21XjD",
        "outputId": "809f8995-7cf9-4dde-bea1-80f50fdc7d1a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "S8UGkINi1Y8H"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYZWUBn41fs-",
        "outputId": "2ddafde7-fb6e-4b32-faab-22e7efd8f7d5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6667, 1.3333, 2.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Clearning Grads\n",
        "\n",
        "Usually there would be a multiple backward pass, but your gradients get accumulated. We can see in below example how this accumualtes and it might be problem getting the actual desired values.\n",
        "\n"
      ],
      "metadata": {
        "id": "5B853Zfs1iar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1maMp9mr2NsF",
        "outputId": "c020bde0-e40f-42dd-ebc3-44df0fd47ae1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2., requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14uKLvEL2WnA",
        "outputId": "d9da9fc4-e30b-4d47-c129-b55176e744f1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st backward pass\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFA6vgX22YOY",
        "outputId": "c97f0d32-131b-4a1d-e687-f1e80e57c44e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# again for second pass\n",
        "y = x ** 2\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uof3CVp03F77",
        "outputId": "142a937a-5154-43df-d401-6fd83b33408f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4., grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now say there is 2nd backward pass, the value should have been same for x.grad i.e. 4. But we get different values becuase of grad accumulation\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vkH1C9w2hPs",
        "outputId": "e12d9a55-6e43-4c50-bb9f-bf1862e9546a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To disable Gradient tracking there are few techniques we can implement\n",
        "\n",
        "# option 1 - requires_grad_(False)\n",
        "# option 2 - detach()\n",
        "# option 3 - torch.no_grad()"
      ],
      "metadata": {
        "id": "7GtPcfM_2wKN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rf6XbiW73r2o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}