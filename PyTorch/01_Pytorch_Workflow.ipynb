{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAJ2YK3d0YU5QrQUym/m4H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Tz5MAp4dXNJq","executionInfo":{"status":"ok","timestamp":1707323835523,"user_tz":360,"elapsed":4967,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"outputs":[],"source":["import torch\n","from torch import nn"]},{"cell_type":"code","source":[],"metadata":{"id":"pweL4iE1ATPT","executionInfo":{"status":"ok","timestamp":1707323835524,"user_tz":360,"elapsed":6,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Preparing Training Data"],"metadata":{"id":"NuPxqbxWcbns"}},{"cell_type":"code","source":["weight = 0.7\n","bias = 0.3\n","\n","start = 0\n","end = 1\n","step = 0.02\n","\n","X = torch.arange(start, end, step=step).unsqueeze(dim=1)\n","y = weight * X + bias\n","\n","X[:10], y[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEucCpJ7cr6u","executionInfo":{"status":"ok","timestamp":1707323835793,"user_tz":360,"elapsed":273,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"5e887b0d-7f01-4e54-8101-b562f8338695"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.0000],\n","         [0.0200],\n","         [0.0400],\n","         [0.0600],\n","         [0.0800],\n","         [0.1000],\n","         [0.1200],\n","         [0.1400],\n","         [0.1600],\n","         [0.1800]]),\n"," tensor([[0.3000],\n","         [0.3140],\n","         [0.3280],\n","         [0.3420],\n","         [0.3560],\n","         [0.3700],\n","         [0.3840],\n","         [0.3980],\n","         [0.4120],\n","         [0.4260]]))"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["## Train Test Split\n","\n","train_split = int(0.8 * len(X))\n","\n","X_train, y_train = X[:train_split], y[:train_split]\n","X_test , y_test = X[train_split:], y[train_split:]\n","\n","len(X_train), len(y_train), len(X_test), len(y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jGXOmIdJcsJT","executionInfo":{"status":"ok","timestamp":1707323835794,"user_tz":360,"elapsed":21,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"dff09e59-8835-4305-cf12-1b1081790325"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(40, 40, 10, 10)"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## Creating Linear Regression"],"metadata":{"id":"vfNfzcmwAmYX"}},{"cell_type":"code","source":["class LinearRegressionModel(nn.Module):\n","\n","  def __init__(self):\n","    super().__init__()\n","\n","    self.weight = nn.Parameter(torch.rand(1,\n","                                          requires_grad=True,\n","                                          dtype=torch.float))\n","\n","    self.bias = nn.Parameter(torch.rand(1,\n","                                          requires_grad=True,\n","                                          dtype=torch.float))\n","\n","    # Write the forward method to define the computation of the model\n","  def forward(self, x : torch.Tensor) -> torch.Tensor:\n","    return self.weight * x + self.bias"],"metadata":{"id":"R0R-M1zdApPF","executionInfo":{"status":"ok","timestamp":1707323835794,"user_tz":360,"elapsed":19,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(42)\n","model_0 = LinearRegressionModel()\n","model_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KrKWiUgqOYkA","executionInfo":{"status":"ok","timestamp":1707323835794,"user_tz":360,"elapsed":18,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"6503c32e-5e56-4a8a-8c21-cad2724cb605"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LinearRegressionModel()"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["list(model_0.parameters())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7QbcF39OZPs","executionInfo":{"status":"ok","timestamp":1707323835794,"user_tz":360,"elapsed":16,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"01fea935-2825-484d-9d30-92f921d68323"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([0.8823], requires_grad=True),\n"," Parameter containing:\n"," tensor([0.9150], requires_grad=True)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWoJkeuqOZZk","executionInfo":{"status":"ok","timestamp":1707323835794,"user_tz":360,"elapsed":13,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"8a911c79-541b-4f1e-be28-6ffb1e39f336"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('weight', tensor([0.8823])), ('bias', tensor([0.9150]))])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["y_pred = model_0(X_test)\n","y_pred\n","# If we see the result, when we call model_0 directly, it uses Backward prapogation wherein to predict the values we don't need any bkwrd prapogations.\n","# Hence we use torch.inference_model() or torch.no_grad() while predicting, as given below."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23RLnrC-OZcw","executionInfo":{"status":"ok","timestamp":1707323835794,"user_tz":360,"elapsed":11,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"be979c6c-f82c-49ce-c763-0542cd0a51d6"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.6208],\n","        [1.6385],\n","        [1.6561],\n","        [1.6738],\n","        [1.6914],\n","        [1.7090],\n","        [1.7267],\n","        [1.7443],\n","        [1.7620],\n","        [1.7796]], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["X_test, y_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgiDWXjjfGQU","executionInfo":{"status":"ok","timestamp":1707323835795,"user_tz":360,"elapsed":10,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"bd1f6d1a-db31-40e3-c8fa-8921607ce2f3"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.8000],\n","         [0.8200],\n","         [0.8400],\n","         [0.8600],\n","         [0.8800],\n","         [0.9000],\n","         [0.9200],\n","         [0.9400],\n","         [0.9600],\n","         [0.9800]]),\n"," tensor([[0.8600],\n","         [0.8740],\n","         [0.8880],\n","         [0.9020],\n","         [0.9160],\n","         [0.9300],\n","         [0.9440],\n","         [0.9580],\n","         [0.9720],\n","         [0.9860]]))"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Lets make Predictions through Inference mode.\n","# Inference mode does not use/perform gradient dicent while predicting because we do not need any gradient decent to happen when doing prediction. Its only needed while training.\n","# This is same as no_grad\n","\n","with torch.inference_mode():\n","  y_pred = model_0(X_test)\n","\n","y_pred\n","\n","# If we look at the predicted outputs with the actual y_test values, then this differes a lot. Why?\n","# Because we have not yet trained the model that would have used gradient decent and backward prapogation, to adjust and get proper weights and biases.\n","# Here the Prediction is directly done through random weights and biases thats get initialized when the LinearRegressionModel is called."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikFO1vXDOZga","executionInfo":{"status":"ok","timestamp":1707323835795,"user_tz":360,"elapsed":8,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"a947043e-ed19-4e6c-f5ab-4a3d41d89e40"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.6208],\n","        [1.6385],\n","        [1.6561],\n","        [1.6738],\n","        [1.6914],\n","        [1.7090],\n","        [1.7267],\n","        [1.7443],\n","        [1.7620],\n","        [1.7796]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[],"metadata":{"id":"RNWlXtA6OZj1","executionInfo":{"status":"ok","timestamp":1707323835795,"user_tz":360,"elapsed":6,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["\n","*Lets Add Loss Functions and Optimizers to do the Gradient descent*"],"metadata":{"id":"e9nhaAyEr4nZ"}},{"cell_type":"code","source":["# Loss function initialization\n","loss_fn = nn.L1Loss()\n","\n","# Setup optimizer\n","optimizer = torch.optim.SGD(model_0.parameters(), # We want to optimize parameter of our model\n","                            lr = 0.01) # lr = Learning Rate"],"metadata":{"id":"s4xrkJXUOZqh","executionInfo":{"status":"ok","timestamp":1707323837800,"user_tz":360,"elapsed":2011,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"InfiTo0EOZuS","executionInfo":{"status":"ok","timestamp":1707323837801,"user_tz":360,"elapsed":41,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**Building a Training Loop and a Testing Loop in Pytorch**\n","\n","Few Pointers -\n","* 1. Loop through the data and do ..\n","* 2. Forward pass also called as forward prapogation to make predictions on data\n","* 3. Calculate the loss\n","* 4. Optimizer Zero Grad\n","* 5. Bacxkward Propagation - Loss Backwards to calculate the gradient of each of the parameters of the model\n","* 6. Optimizer step - Here we adjust the parametrs against the loss calculated inorder to improve the loss."],"metadata":{"id":"i5ARUdShSj2U"}},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtAPDNGuiw9g","executionInfo":{"status":"ok","timestamp":1707323837807,"user_tz":360,"elapsed":45,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"91f5a174-3a13-43fa-f13a-8eaac846e893"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('weight', tensor([0.8823])), ('bias', tensor([0.9150]))])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","epochs = 110\n","\n","## Training\n","for epoch in range(epochs):\n","  #print(f\"Epoch : {epoch}\")\n","  model_0.train() # This sets mode to train. Explaination given below cell\n","\n","  #2. Forward pass\n","  y_pred = model_0(X_train)\n","\n","  #3. Calulate the Loss\n","  loss = loss_fn(y_pred, y_train)\n","  #print(f\"Loss : {loss}\")\n","\n","\n","  #4. Optimizer Zero grad\n","  optimizer.zero_grad()     #  usually optimizer.zero_grad() should always be invoked before loss.backward() and not in between loss.backward() and optimizer.step()\n","\n","  #5. Perform backpropagation on the loss w.r.t parameters of the model\n","  loss.backward()\n","\n","  #6. Step the optimizer\n","  optimizer.step()\n","\n","\n","\n","  ## Testing -\n","  model_0.eval() # Turn off different setting in the model not needed for evaluation/testing\n","  with torch.inference_mode(): #Inference mode does not use/perform gradient dicent while predicting because we do not need any gradient decent to happen when doing prediction. Its only needed while training.\n","# This is same as with torch.no_grad():\n","    # do the forward pass\n","    test_pred = model_0(X_test)\n","\n","    # calculate the test loss\n","    test_loss = loss_fn(test_pred, y_test)\n","\n","  if epoch % 10 == 0:\n","    print(f\"Epoch : {epoch} , Train Loss : {loss}, Test Loss : {test_loss}\")\n","    print(f\"Parameters : {model_0.state_dict()} \\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0ngwEZpXnwx","executionInfo":{"status":"ok","timestamp":1707323837943,"user_tz":360,"elapsed":166,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"55a76235-cad4-462d-c129-a74c2d8e6ce6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch : 0 , Train Loss : 0.6860889196395874, Test Loss : 0.7637526988983154\n","Parameters : OrderedDict([('weight', tensor([0.8784])), ('bias', tensor([0.9050]))]) \n","\n","Epoch : 10 , Train Loss : 0.5708791017532349, Test Loss : 0.6290428042411804\n","Parameters : OrderedDict([('weight', tensor([0.8394])), ('bias', tensor([0.8050]))]) \n","\n","Epoch : 20 , Train Loss : 0.45566922426223755, Test Loss : 0.4943329691886902\n","Parameters : OrderedDict([('weight', tensor([0.8004])), ('bias', tensor([0.7050]))]) \n","\n","Epoch : 30 , Train Loss : 0.34045934677124023, Test Loss : 0.35962313413619995\n","Parameters : OrderedDict([('weight', tensor([0.7614])), ('bias', tensor([0.6050]))]) \n","\n","Epoch : 40 , Train Loss : 0.2252494841814041, Test Loss : 0.2249133139848709\n","Parameters : OrderedDict([('weight', tensor([0.7224])), ('bias', tensor([0.5050]))]) \n","\n","Epoch : 50 , Train Loss : 0.1100396141409874, Test Loss : 0.09020347893238068\n","Parameters : OrderedDict([('weight', tensor([0.6834])), ('bias', tensor([0.4050]))]) \n","\n","Epoch : 60 , Train Loss : 0.009724985808134079, Test Loss : 0.020998019725084305\n","Parameters : OrderedDict([('weight', tensor([0.6539])), ('bias', tensor([0.3200]))]) \n","\n","Epoch : 70 , Train Loss : 0.006216754671186209, Test Loss : 0.014099234715104103\n","Parameters : OrderedDict([('weight', tensor([0.6707])), ('bias', tensor([0.3120]))]) \n","\n","Epoch : 80 , Train Loss : 0.002788322512060404, Test Loss : 0.005826681852340698\n","Parameters : OrderedDict([('weight', tensor([0.6878])), ('bias', tensor([0.3050]))]) \n","\n","Epoch : 90 , Train Loss : 0.007095950655639172, Test Loss : 0.00754010071977973\n","Parameters : OrderedDict([('weight', tensor([0.6938])), ('bias', tensor([0.2980]))]) \n","\n","Epoch : 100 , Train Loss : 0.007095950655639172, Test Loss : 0.00754010071977973\n","Parameters : OrderedDict([('weight', tensor([0.6938])), ('bias', tensor([0.2980]))]) \n","\n"]}]},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6SFmnIPJ1aU5","executionInfo":{"status":"ok","timestamp":1707323837944,"user_tz":360,"elapsed":30,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"3035e6c4-c129-4816-a417-6658e74c79ca"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('weight', tensor([0.6977])), ('bias', tensor([0.3080]))])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["with torch.inference_mode():\n","  y_pred_new = model_0(X_test)\n","\n","y_pred_new"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7yZxgY8YpA4","executionInfo":{"status":"ok","timestamp":1707323837944,"user_tz":360,"elapsed":22,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"e4ac28ca-6626-4ec2-b69f-912f68c1ea3f"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8661],\n","        [0.8801],\n","        [0.8940],\n","        [0.9080],\n","        [0.9220],\n","        [0.9359],\n","        [0.9499],\n","        [0.9638],\n","        [0.9778],\n","        [0.9917]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["#Comparing with original y_test\n","y_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GVU3GwEM1wEO","executionInfo":{"status":"ok","timestamp":1707323837944,"user_tz":360,"elapsed":17,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"e18d089d-8d64-4024-ca05-5b2989fc4a09"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8600],\n","        [0.8740],\n","        [0.8880],\n","        [0.9020],\n","        [0.9160],\n","        [0.9300],\n","        [0.9440],\n","        [0.9580],\n","        [0.9720],\n","        [0.9860]])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["\n","**1. Why we use model.train()**\n","\n","--> *model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.*\n","\n","*More details: model.train() sets the mode to train. You can call either model.eval() or model.train(mode=False) to tell that you are testing. It is somewhat intuitive to expect train function to train model but it does not do that. It just sets the mode.*\n","\n","Source - https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch/51433411#51433411?newreg=d75231dcb3114a89bad77134a7d8c2f1\n","\n","**2. About optimizer.zero_grad()**\n","\n","--> *optimizer.zero_grad() should always be invoked before loss.backward(). This ensures that the gradients are properly zeroed out and then computed and stored in the appropriate tensors' grad field*\n","\n","https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n","\n","https://datascience.stackexchange.com/questions/124487/what-do-we-mean-by-optimizer-zero-grad\n","\n","https://github.com/yunjey/pytorch-tutorial/issues/238\n","\n","\n","**3. About model.eval()**\n","\n","--> *model.eval() is a kind of switch for some specific layers/parts of the model that behave differently during training and inference (evaluating) time. For example, Dropouts Layers, BatchNorm Layers etc. You need to turn them off during model evaluation, and .eval() will do it for you. In addition, the common practice for evaluating/validation is using torch.no_grad() in pair with model.eval() to turn off gradients computation*\n"],"metadata":{"id":"ucvVrN3RYqHx"}},{"cell_type":"markdown","source":["###Saving The Model\n","\n","There are 3 methodes used in saving the model and unloading-\n","1. torch.save() - saves Pytorch object in Python pickle form\n","2. torch.load() - allows you to load a saved Pytorch Object\n","3. torch.nn.Module.load_state_dict() - This allows to load a model's saved state dictionary.\n","\n","Pytorch Save and load model : - https://pytorch.org/tutorials/beginner/saving_loading_models.html"],"metadata":{"id":"-i4RlLoahzsx"}},{"cell_type":"code","source":["# Saving our Pytorch model\n","\n","from pathlib import Path\n","\n","#1. Create models directory\n","MODEL_PATH = Path(\"models\")\n","MODEL_PATH.mkdir(parents=True, exist_ok=\"True\")\n","\n","#2. Create model save path\n","MODEL_NAME = \"01_pytorch_workflow.pth\" # we can use .pth or .pt to save the PyTorch objects\n","MODEL_SAVE_PATH = MODEL_PATH/MODEL_NAME\n","print(MODEL_SAVE_PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BVzLACZYpHX","executionInfo":{"status":"ok","timestamp":1707323837944,"user_tz":360,"elapsed":15,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"e2b43da6-414c-4ce8-ee2d-6ffa5fb9a281"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["models/01_pytorch_workflow.pth\n"]}]},{"cell_type":"code","source":["#3. Save the model\n","torch.save(obj=model_0.state_dict(),\n","           f = MODEL_SAVE_PATH)"],"metadata":{"id":"meQeDLsxYpKS","executionInfo":{"status":"ok","timestamp":1707323837944,"user_tz":360,"elapsed":13,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["model_0.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TDTe8yvqiMvP","executionInfo":{"status":"ok","timestamp":1707323837947,"user_tz":360,"elapsed":15,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"b94a0e65-dfbb-409a-cd50-c8c1ce8fad38"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('weight', tensor([0.6977])), ('bias', tensor([0.3080]))])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["!ls -l models"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_gGxjphYpOA","executionInfo":{"status":"ok","timestamp":1707323838099,"user_tz":360,"elapsed":164,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"25975eb9-9de3-4a65-cd03-26321a98705f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["total 4\n","-rw-r--r-- 1 root root 1568 Feb  7 16:37 01_pytorch_workflow.pth\n"]}]},{"cell_type":"code","source":["\n"],"metadata":{"id":"qUtPx-nfZyJD","executionInfo":{"status":"ok","timestamp":1707323838100,"user_tz":360,"elapsed":10,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["\n"," **Loading the Model to the instance**"],"metadata":{"id":"TvaiENBBiUuM"}},{"cell_type":"code","source":["# Loading the Model to the instance.\n","# First we need to create an instance of the Algorithm/model we are trying to create.\n","\n","loaded_model_0 = LinearRegressionModel()\n","\n","# Load the saved state_dict of model_0 to the instance created above (loaded_model_0)\n","\n","loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"febR0Y8-icjA","executionInfo":{"status":"ok","timestamp":1707323838100,"user_tz":360,"elapsed":9,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"caef79c8-4395-40d7-da7c-606f57aead5b"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["loaded_model_0.state_dict()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljW89ss2jSp6","executionInfo":{"status":"ok","timestamp":1707323838100,"user_tz":360,"elapsed":6,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"01890c49-f762-488f-860b-97cea9d99ce8"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([('weight', tensor([0.6977])), ('bias', tensor([0.3080]))])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# Lets make some predictions\n","# erlier predictions were -\n","print(f\"Earlier Predictions on X_test are : \\n {y_pred_new}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIi_Kj9HjjOw","executionInfo":{"status":"ok","timestamp":1707323838100,"user_tz":360,"elapsed":5,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"0097d290-39b3-4e76-f4f3-e60ab2dea183"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Earlier Predictions on X_test are : \n"," tensor([[0.8661],\n","        [0.8801],\n","        [0.8940],\n","        [0.9080],\n","        [0.9220],\n","        [0.9359],\n","        [0.9499],\n","        [0.9638],\n","        [0.9778],\n","        [0.9917]])\n"]}]},{"cell_type":"code","source":["# Make prediction from the model that was save and then loaded back\n","loaded_model_0.eval()\n","with torch.inference_mode():\n","  loaded_model_pred = loaded_model_0(X_test)\n","loaded_model_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LL3s3j3HkaXj","executionInfo":{"status":"ok","timestamp":1707323838253,"user_tz":360,"elapsed":156,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"aa8f9acd-402f-4a17-ce00-e807e256b87e"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8661],\n","        [0.8801],\n","        [0.8940],\n","        [0.9080],\n","        [0.9220],\n","        [0.9359],\n","        [0.9499],\n","        [0.9638],\n","        [0.9778],\n","        [0.9917]])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["y_pred_new == loaded_model_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-A0J_7lKaEgB","executionInfo":{"status":"ok","timestamp":1707323838253,"user_tz":360,"elapsed":6,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}},"outputId":"22905156-ce4b-4dda-fcce-8e3ea6b80fef"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[True],\n","        [True],\n","        [True],\n","        [True],\n","        [True],\n","        [True],\n","        [True],\n","        [True],\n","        [True],\n","        [True]])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":[],"metadata":{"id":"cbmPTcWXaM9h","executionInfo":{"status":"ok","timestamp":1707323838253,"user_tz":360,"elapsed":4,"user":{"displayName":"Swapnil Nandanwar","userId":"10260422342272113088"}}},"execution_count":25,"outputs":[]}]}