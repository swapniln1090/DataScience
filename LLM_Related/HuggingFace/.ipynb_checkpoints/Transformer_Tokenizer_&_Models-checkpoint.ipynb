{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abb6df5-6b1f-43f5-89b9-cb0180269ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754b30f2-f2f2-46c4-bb88-5d1066516713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snandanw\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fa85c9-06b3-4909-92f4-91183899f86d",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad698fe-220e-4159-810a-a6c942deb1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Roses': 17303,\n",
       " 'hitting': 6886,\n",
       " '216': 22148,\n",
       " 'phenomena': 14343,\n",
       " 'cricketer': 9469,\n",
       " 'Top': 3299,\n",
       " 'wholly': 12907,\n",
       " 'crossed': 3809,\n",
       " 'Hatch': 25945,\n",
       " '##14': 17175,\n",
       " 'Mitchell': 5741,\n",
       " '##hta': 26489,\n",
       " 'appears': 2691,\n",
       " '##aw': 7220,\n",
       " '##leading': 28007,\n",
       " '##jana': 21026,\n",
       " 'Hastings': 12446,\n",
       " 'worm': 19686,\n",
       " 'Tang': 10215,\n",
       " 'quietly': 4432,\n",
       " 'Rolling': 8782,\n",
       " 'altitudes': 24604,\n",
       " '##payers': 27452,\n",
       " 'Scotland': 3030,\n",
       " 'rat': 11631,\n",
       " 'handle': 4282,\n",
       " 'ordination': 20424,\n",
       " 'scrambled': 13988,\n",
       " 'chords': 22098,\n",
       " 'Burma': 11023,\n",
       " 'DB': 24044,\n",
       " '##ga': 2571,\n",
       " 'Video': 6301,\n",
       " 'Inside': 7323,\n",
       " 'resident': 6408,\n",
       " 'Calendar': 26208,\n",
       " '##cola': 19673,\n",
       " '##ella': 7772,\n",
       " 'vast': 6047,\n",
       " 'dwell': 26812,\n",
       " 'Koreans': 27757,\n",
       " 'inspection': 11820,\n",
       " 'Seymour': 13572,\n",
       " 'entertainment': 5936,\n",
       " 'Neptune': 23405,\n",
       " 'Mariano': 25260,\n",
       " 'headed': 2917,\n",
       " 'subdivisions': 25238,\n",
       " '1769': 22632,\n",
       " '##pical': 15328,\n",
       " '##ffing': 17242,\n",
       " 'Season': 5623,\n",
       " 'collections': 6286,\n",
       " 'signature': 8250,\n",
       " '##vocative': 22738,\n",
       " 'When': 1332,\n",
       " '##unted': 13153,\n",
       " 'plastered': 24304,\n",
       " 'alternate': 7551,\n",
       " '1883': 6720,\n",
       " 'drone': 22020,\n",
       " 'Oct': 14125,\n",
       " 'l': 181,\n",
       " '##ca': 2599,\n",
       " 'Atkins': 23196,\n",
       " 'Eleven': 15159,\n",
       " 'seeing': 3195,\n",
       " '##po': 5674,\n",
       " 'subtle': 11515,\n",
       " 'perished': 25996,\n",
       " 'disliked': 20604,\n",
       " 'Pink': 10763,\n",
       " 'Sterling': 12319,\n",
       " '##tine': 12569,\n",
       " 'viable': 15668,\n",
       " '##leaf': 21407,\n",
       " '##lice': 11815,\n",
       " 'pursed': 24292,\n",
       " 'expression': 2838,\n",
       " 'vampire': 3980,\n",
       " 'countryside': 11408,\n",
       " '[unused96]': 96,\n",
       " '##usage': 27130,\n",
       " 'Tel': 11341,\n",
       " 'pavement': 15823,\n",
       " 'prostitute': 21803,\n",
       " 'Therapy': 23789,\n",
       " 'Ezio': 25689,\n",
       " 'humanities': 26419,\n",
       " 'beats': 11883,\n",
       " 'Pepper': 19278,\n",
       " 'wave': 4003,\n",
       " 'literally': 6290,\n",
       " 'exceptional': 12688,\n",
       " '##eng': 14429,\n",
       " 'Marks': 16875,\n",
       " 'publicly': 6783,\n",
       " 'Congressman': 15444,\n",
       " 'Ghost': 9040,\n",
       " 'choice': 3026,\n",
       " '##lins': 17828,\n",
       " '##fall': 8877,\n",
       " 'notice': 4430,\n",
       " 'stone': 2576,\n",
       " 'knights': 15480,\n",
       " 'pursued': 9281,\n",
       " '##nose': 22583,\n",
       " 'chemicals': 13558,\n",
       " 'tomb': 8880,\n",
       " 'suite': 9555,\n",
       " 'carriage': 9362,\n",
       " 'Denise': 16160,\n",
       " 'frog': 13670,\n",
       " 'inflation': 15503,\n",
       " '##mons': 17199,\n",
       " 'churchyard': 24701,\n",
       " 'Lenny': 20740,\n",
       " '##heat': 25162,\n",
       " 'improvements': 8313,\n",
       " 'Troy': 9454,\n",
       " 'spirit': 4840,\n",
       " 'freaking': 14599,\n",
       " 'maps': 7415,\n",
       " 'tying': 16366,\n",
       " 'monitors': 16884,\n",
       " '184': 21421,\n",
       " 'Publisher': 26045,\n",
       " 'Egyptians': 26145,\n",
       " 'creative': 6228,\n",
       " 'Corey': 19521,\n",
       " 'accompany': 12910,\n",
       " 'troll': 28000,\n",
       " 'Wexford': 25783,\n",
       " '##art': 9349,\n",
       " '##nian': 11091,\n",
       " 'angrily': 14887,\n",
       " 'era': 3386,\n",
       " '##ș': 24158,\n",
       " '##rges': 24173,\n",
       " 'ウ': 934,\n",
       " 'north': 1564,\n",
       " 'exist': 4056,\n",
       " 'Annual': 8451,\n",
       " '##zhou': 10753,\n",
       " 'Nam': 19346,\n",
       " '36': 3164,\n",
       " 'caucus': 27690,\n",
       " 'dealt': 9130,\n",
       " 'Achievement': 10295,\n",
       " '##ctic': 11143,\n",
       " 'inlet': 27036,\n",
       " '##aria': 11315,\n",
       " 'ban': 8214,\n",
       " 'mused': 24807,\n",
       " '##champ': 23542,\n",
       " 'stark': 18237,\n",
       " 'nevertheless': 12341,\n",
       " 'Telephone': 22580,\n",
       " '##ville': 2138,\n",
       " '##onal': 24059,\n",
       " 'dictator': 26400,\n",
       " 'medicine': 5182,\n",
       " 'Scott': 2796,\n",
       " 'McGregor': 25812,\n",
       " '##med': 4611,\n",
       " 'Moldova': 17877,\n",
       " '1944': 2782,\n",
       " 'inherit': 26047,\n",
       " 'crowd': 3515,\n",
       " 'lengths': 10707,\n",
       " 'girlfriend': 6124,\n",
       " 'rumor': 24206,\n",
       " 'represents': 5149,\n",
       " '##ח': 28452,\n",
       " 'Breakfast': 20289,\n",
       " '##ַ': 28441,\n",
       " 'ظ': 582,\n",
       " 'many': 1242,\n",
       " '##vous': 11944,\n",
       " 'Primary': 6933,\n",
       " 'advertisements': 15448,\n",
       " 'Handicap': 25597,\n",
       " 'presentations': 20531,\n",
       " 'Constituency': 21080,\n",
       " 'positive': 3112,\n",
       " 'pioneering': 14024,\n",
       " 'cavalry': 7915,\n",
       " 'later': 1224,\n",
       " 'age': 1425,\n",
       " 'trend': 10209,\n",
       " 'bargaining': 25656,\n",
       " 'winding': 14042,\n",
       " 'burying': 25317,\n",
       " 'woken': 25580,\n",
       " 'more': 1167,\n",
       " '##ep': 8043,\n",
       " 'import': 13757,\n",
       " 'Xbox': 12335,\n",
       " 'entities': 11659,\n",
       " 'release': 1836,\n",
       " 'slavery': 9401,\n",
       " 'predators': 15334,\n",
       " 'decreasing': 18326,\n",
       " 'assume': 7568,\n",
       " '##sound': 22909,\n",
       " 'ë': 257,\n",
       " 'English': 1483,\n",
       " 'specific': 2747,\n",
       " 'column': 5551,\n",
       " '[unused88]': 88,\n",
       " 'Irene': 13524,\n",
       " 'belt': 5614,\n",
       " 'Lady': 2876,\n",
       " 'locking': 16348,\n",
       " 'associates': 13681,\n",
       " '##Ḩ': 28631,\n",
       " 'skating': 12718,\n",
       " 'forgive': 10737,\n",
       " 'Mesa': 18506,\n",
       " 'Is': 2181,\n",
       " '##irt': 25074,\n",
       " '1927': 3951,\n",
       " 'carried': 2446,\n",
       " 'absorption': 18099,\n",
       " '##rik': 10160,\n",
       " 'Carl': 4804,\n",
       " 'canal': 7684,\n",
       " '##aves': 13799,\n",
       " 'ears': 4942,\n",
       " 'm³': 14813,\n",
       " '##nies': 16133,\n",
       " 'Perez': 20007,\n",
       " 'theory': 2749,\n",
       " 'Papers': 19023,\n",
       " 'Gilmore': 25624,\n",
       " '##oral': 17536,\n",
       " 'seniors': 20566,\n",
       " 'Resources': 9868,\n",
       " 'Abbas': 19166,\n",
       " 'representative': 4702,\n",
       " 'source': 2674,\n",
       " '##V': 2559,\n",
       " 'collapse': 7546,\n",
       " 'sole': 6753,\n",
       " 'unusually': 14624,\n",
       " 'evolving': 23657,\n",
       " 'Musical': 8660,\n",
       " 'Pretty': 12004,\n",
       " 'Associates': 15264,\n",
       " 'accordingly': 17472,\n",
       " 'picturesque': 27667,\n",
       " 'Fleming': 14217,\n",
       " '##burne': 26980,\n",
       " 'vocational': 18341,\n",
       " 'burst': 6007,\n",
       " 'Donovan': 12930,\n",
       " '80': 2908,\n",
       " 'ion': 14469,\n",
       " 'splash': 24194,\n",
       " 'results': 2686,\n",
       " '##ination': 9400,\n",
       " 'luggage': 20307,\n",
       " '##crates': 24460,\n",
       " 'complementary': 24671,\n",
       " '##ule': 8722,\n",
       " 'Uganda': 9831,\n",
       " '##ver': 4121,\n",
       " 'orphan': 25298,\n",
       " 'submarines': 13191,\n",
       " '##zee': 26328,\n",
       " 'prominence': 12299,\n",
       " '##jure': 28014,\n",
       " 'engage': 8306,\n",
       " 'Domesday': 24871,\n",
       " 'papers': 4580,\n",
       " 'Gaston': 21668,\n",
       " 'Organization': 6534,\n",
       " 'seem': 3166,\n",
       " 'Bengals': 25884,\n",
       " 'captains': 20672,\n",
       " 'Namibia': 16228,\n",
       " 'changes': 2607,\n",
       " 'baking': 26377,\n",
       " '##bearable': 27244,\n",
       " 'streets': 4324,\n",
       " '##uma': 10161,\n",
       " '##)': 28133,\n",
       " 'Wrong': 20337,\n",
       " '##rva': 13461,\n",
       " '金': 1079,\n",
       " 'chaos': 10676,\n",
       " 'rag': 26133,\n",
       " 'affair': 7033,\n",
       " 'letter': 2998,\n",
       " 'Simmons': 14068,\n",
       " 'enclosure': 19904,\n",
       " '##uli': 15818,\n",
       " 'trustee': 17665,\n",
       " '##worth': 4189,\n",
       " 'divided': 3233,\n",
       " 'Jets': 13136,\n",
       " '125': 8347,\n",
       " 'nets': 26188,\n",
       " 'tenants': 16086,\n",
       " 'remastered': 23109,\n",
       " '##犬': 28947,\n",
       " '##書': 28929,\n",
       " 'Users': 24222,\n",
       " 'society': 2808,\n",
       " 'Phelps': 22385,\n",
       " 'veil': 20945,\n",
       " 'coefficient': 21130,\n",
       " 'economy': 4190,\n",
       " '##bed': 4774,\n",
       " '##nut': 12251,\n",
       " '##lena': 23675,\n",
       " 'Ke': 26835,\n",
       " '##lanche': 27474,\n",
       " 'varying': 9507,\n",
       " 'Tales': 9931,\n",
       " 'Franks': 23653,\n",
       " 'frost': 26109,\n",
       " '##cope': 16260,\n",
       " 'updated': 8054,\n",
       " 'today': 2052,\n",
       " 'brand': 4097,\n",
       " 'Napoleon': 9006,\n",
       " 'Crazy': 11722,\n",
       " 'Move': 15729,\n",
       " 'Eye': 9329,\n",
       " '##ivers': 25105,\n",
       " 'Ν': 405,\n",
       " 'halftime': 26077,\n",
       " 'Sara': 6936,\n",
       " 'Automobile': 27336,\n",
       " '##18': 15292,\n",
       " 'group': 1372,\n",
       " 'therapy': 7606,\n",
       " 'financially': 14396,\n",
       " 'Chorus': 16415,\n",
       " 'に': 914,\n",
       " 'Citizen': 15783,\n",
       " '##س': 28484,\n",
       " 'reunion': 14150,\n",
       " 'fragment': 17906,\n",
       " 'Allah': 18203,\n",
       " 'overseen': 25197,\n",
       " 'Lehigh': 25912,\n",
       " 'abandoning': 22634,\n",
       " 'alleged': 6351,\n",
       " 'gang': 6939,\n",
       " '##fied': 8971,\n",
       " 'Wilson': 3425,\n",
       " 'comedy': 3789,\n",
       " 'Gloria': 11024,\n",
       " '##ars': 7666,\n",
       " 'blazing': 20013,\n",
       " '##ʻ': 28301,\n",
       " '##solation': 27959,\n",
       " 'some': 1199,\n",
       " 'McGrath': 25975,\n",
       " 'Borders': 25746,\n",
       " 'comb': 27481,\n",
       " '##ett': 5912,\n",
       " 'wire': 7700,\n",
       " 'Blake': 5887,\n",
       " 'tails': 21315,\n",
       " 'ル': 970,\n",
       " 'bomb': 5985,\n",
       " 'returns': 5166,\n",
       " 'death': 1473,\n",
       " '##ress': 7370,\n",
       " 'attractive': 8394,\n",
       " 'Salzburg': 19802,\n",
       " 'Shapiro': 27747,\n",
       " 'invading': 19185,\n",
       " '##cas': 21995,\n",
       " 'Khorasan': 25961,\n",
       " 'GA': 20173,\n",
       " 'radioactive': 21156,\n",
       " 'bony': 26647,\n",
       " 'coverage': 5811,\n",
       " 'justification': 22647,\n",
       " 'Plant': 9646,\n",
       " 'paramilitary': 25349,\n",
       " 'racial': 5209,\n",
       " 'chase': 9839,\n",
       " 'traces': 10749,\n",
       " 'Collegiate': 14329,\n",
       " 'deaf': 16814,\n",
       " 'distinctive': 7884,\n",
       " 'booking': 26059,\n",
       " 'farmland': 17790,\n",
       " '##qualified': 25005,\n",
       " 'mainline': 25229,\n",
       " 'among': 1621,\n",
       " 'Journey': 12015,\n",
       " 'Collective': 22380,\n",
       " '270': 14162,\n",
       " 'Minority': 26495,\n",
       " 'suitcase': 17655,\n",
       " 'otherwise': 4303,\n",
       " '##dates': 20388,\n",
       " '##mered': 18583,\n",
       " 'Cycle': 19991,\n",
       " '56': 4376,\n",
       " 'schooling': 15113,\n",
       " 'users': 4713,\n",
       " 'jobs': 5448,\n",
       " 'listened': 7327,\n",
       " '##set': 9388,\n",
       " 'CDs': 16881,\n",
       " 'code': 3463,\n",
       " 'Battle': 2651,\n",
       " 'Golf': 8206,\n",
       " 'Jew': 17706,\n",
       " 'Teachers': 14290,\n",
       " '##roup': 24220,\n",
       " 'Boom': 19287,\n",
       " 'liquid': 6161,\n",
       " 'Sonia': 17450,\n",
       " 'strap': 17896,\n",
       " 'nucleus': 14297,\n",
       " 'Microsoft': 6998,\n",
       " 'momentarily': 17429,\n",
       " 'armed': 4223,\n",
       " 'beads': 20108,\n",
       " '##igan': 10888,\n",
       " 'DS': 18448,\n",
       " '##lumber': 26993,\n",
       " 'Mustafa': 21638,\n",
       " 'Labour': 4560,\n",
       " 'developmental': 16700,\n",
       " '##gene': 27054,\n",
       " '##₈': 28715,\n",
       " 'Serra': 25605,\n",
       " 'Finalist': 24814,\n",
       " 'advocate': 9411,\n",
       " 'Swansea': 17057,\n",
       " '##ys': 6834,\n",
       " 'Islanders': 21574,\n",
       " '2015': 1410,\n",
       " 'couldn': 1577,\n",
       " 'Rams': 15183,\n",
       " 'slow': 3345,\n",
       " 'waving': 12502,\n",
       " 'folding': 13748,\n",
       " 'CC': 21362,\n",
       " 'nationalism': 16767,\n",
       " '##quities': 20030,\n",
       " 'expectations': 11471,\n",
       " 'Preliminary': 26308,\n",
       " 'ACC': 18396,\n",
       " 'suspects': 13928,\n",
       " 'Hilda': 23927,\n",
       " 'sexual': 3785,\n",
       " 'doomed': 23263,\n",
       " 'year': 1214,\n",
       " 'Franciscan': 21183,\n",
       " 'matched': 10260,\n",
       " 'chips': 13228,\n",
       " '[unused87]': 87,\n",
       " 'Victor': 4622,\n",
       " 'recipients': 18509,\n",
       " 'contracting': 26706,\n",
       " 'prohibited': 11018,\n",
       " 'traders': 14552,\n",
       " 'company': 1419,\n",
       " 'Senator': 5293,\n",
       " '##ice': 4396,\n",
       " 'alias': 17486,\n",
       " 'anxiously': 26433,\n",
       " 'Hugh': 6016,\n",
       " '##itz': 8976,\n",
       " '##alia': 19457,\n",
       " 'CR': 15531,\n",
       " '##venting': 27586,\n",
       " 'She': 1153,\n",
       " 'cancellation': 18360,\n",
       " '##yster': 21878,\n",
       " '##إ': 28473,\n",
       " 'ế': 745,\n",
       " 'drives': 9307,\n",
       " '##iro': 9992,\n",
       " '##?': 28145,\n",
       " 'cake': 10851,\n",
       " 'introducing': 11100,\n",
       " '##TE': 12880,\n",
       " 'promise': 4437,\n",
       " 'Pavilion': 16790,\n",
       " '##plicate': 21379,\n",
       " '##pian': 22032,\n",
       " '##tens': 23826,\n",
       " 'mumbled': 11599,\n",
       " '##郎': 28968,\n",
       " 'battery': 7105,\n",
       " 'tower': 3590,\n",
       " 'Pine': 11465,\n",
       " '##voy': 12716,\n",
       " 'younger': 3247,\n",
       " 'к': 485,\n",
       " 'Results': 16005,\n",
       " 'domains': 13770,\n",
       " 'reproduced': 25017,\n",
       " '##arded': 26541,\n",
       " 'watching': 2903,\n",
       " 'Degree': 16861,\n",
       " 'thankful': 21602,\n",
       " 'ħ': 296,\n",
       " 'ruling': 6550,\n",
       " 'parasite': 25706,\n",
       " 'infant': 11551,\n",
       " 'elemental': 24789,\n",
       " '##pressive': 16568,\n",
       " 'locality': 10157,\n",
       " '##scent': 23193,\n",
       " 'occupying': 14854,\n",
       " 'verses': 11808,\n",
       " 'squared': 23215,\n",
       " 'Thing': 11675,\n",
       " 'Vancouver': 6336,\n",
       " 'named': 1417,\n",
       " 'minorities': 16130,\n",
       " 'Teacher': 14208,\n",
       " 'deities': 19018,\n",
       " 'disagreement': 19855,\n",
       " 'confirmation': 15468,\n",
       " 'bacterium': 27136,\n",
       " '##rry': 6234,\n",
       " 'Bit': 27400,\n",
       " 'conglomerate': 27482,\n",
       " 'Lance': 11856,\n",
       " '##cars': 18633,\n",
       " '##ński': 15713,\n",
       " 'Liga': 7837,\n",
       " 'Greenwich': 14323,\n",
       " 'desk': 3917,\n",
       " 'ₐ': 824,\n",
       " 'Skinner': 18835,\n",
       " '##anger': 27290,\n",
       " 'medal': 3086,\n",
       " 'pillar': 18515,\n",
       " '##truder': 24993,\n",
       " 'Meanwhile': 5459,\n",
       " 'Such': 5723,\n",
       " '##iper': 26734,\n",
       " 'Paterson': 20843,\n",
       " 'scored': 2297,\n",
       " 'unlikely': 9803,\n",
       " 'Marino': 18940,\n",
       " 'Far': 8040,\n",
       " 'Wilkins': 25302,\n",
       " 'editions': 7307,\n",
       " 'pronunciation': 17238,\n",
       " '##rries': 15494,\n",
       " 'Polly': 18905,\n",
       " 'handing': 13712,\n",
       " 'ű': 331,\n",
       " 'Cardinals': 9951,\n",
       " 'confronted': 13367,\n",
       " '##lance': 13831,\n",
       " '##icide': 24421,\n",
       " 'consultant': 9496,\n",
       " 'Marsh': 11463,\n",
       " 'Leaves': 21825,\n",
       " '##ნ': 28605,\n",
       " 'stocks': 17901,\n",
       " 'swung': 7185,\n",
       " 'Rd': 23604,\n",
       " 'Benton': 20228,\n",
       " '##eda': 14604,\n",
       " 'borrowed': 12214,\n",
       " 'Liberia': 19427,\n",
       " 'ejected': 25292,\n",
       " '##gration': 18353,\n",
       " 'shaved': 23116,\n",
       " 'commended': 27083,\n",
       " '##settled': 23242,\n",
       " 'ownership': 5582,\n",
       " 'sleek': 23996,\n",
       " '##MM': 25290,\n",
       " '##ի': 28427,\n",
       " 'Rhodes': 10575,\n",
       " '##kura': 24470,\n",
       " 'Idaho': 9559,\n",
       " '星': 1035,\n",
       " '##ów': 6781,\n",
       " 'Majesty': 10637,\n",
       " 'Silence': 12939,\n",
       " 'airline': 8694,\n",
       " 'Seal': 19200,\n",
       " 'tolerant': 26827,\n",
       " 'Paisley': 27771,\n",
       " 'methodology': 18576,\n",
       " 'Action': 6605,\n",
       " 'exclusion': 18434,\n",
       " '##link': 13255,\n",
       " '##rench': 18129,\n",
       " 'vendors': 19086,\n",
       " '##ு': 28584,\n",
       " 'personality': 5935,\n",
       " 'compatibility': 25400,\n",
       " 'French': 1497,\n",
       " 'Into': 14000,\n",
       " '##sonic': 19500,\n",
       " 'Subsequent': 20499,\n",
       " 'beg': 13803,\n",
       " '##laws': 17830,\n",
       " '##rter': 27618,\n",
       " 'document': 5830,\n",
       " 'infectious': 20342,\n",
       " 'Bremen': 17339,\n",
       " 'candle': 15488,\n",
       " 'Tunnel': 12872,\n",
       " 'disturb': 26886,\n",
       " 'Mount': 3572,\n",
       " '##tro': 8005,\n",
       " 'tunnels': 11175,\n",
       " 'Commodore': 14053,\n",
       " '##RE': 16941,\n",
       " '##mism': 20279,\n",
       " 'Qi': 24357,\n",
       " '##ₛ': 28728,\n",
       " 'functioning': 12641,\n",
       " '##ieri': 21328,\n",
       " '##aker': 17051,\n",
       " 'catalyst': 20546,\n",
       " 'Teresa': 12575,\n",
       " 'T': 157,\n",
       " 'seated': 8808,\n",
       " 'collar': 9704,\n",
       " 'Washington': 1994,\n",
       " 'uncredited': 7750,\n",
       " 'camps': 7869,\n",
       " 'Colts': 16186,\n",
       " '##trata': 26649,\n",
       " 'roots': 6176,\n",
       " '##oga': 23282,\n",
       " 'Curran': 21428,\n",
       " '##sko': 20428,\n",
       " 'cubic': 12242,\n",
       " '##lan': 4371,\n",
       " 'shadow': 6464,\n",
       " 'limestone': 10718,\n",
       " 'insulting': 27296,\n",
       " 'tributaries': 17147,\n",
       " 'Psychological': 24797,\n",
       " '##obia': 23145,\n",
       " '##_': 28151,\n",
       " 'total': 1703,\n",
       " 'Abe': 15849,\n",
       " 'hardcover': 27214,\n",
       " 'Cups': 21671,\n",
       " 'discouraged': 25761,\n",
       " 'columns': 7411,\n",
       " '##zy': 6482,\n",
       " '##emption': 19007,\n",
       " '##ille': 8683,\n",
       " 'Franco': 9063,\n",
       " 'Whenever': 20589,\n",
       " 'NSW': 11557,\n",
       " '##5': 1571,\n",
       " '##ples': 15377,\n",
       " 'grants': 9295,\n",
       " 'securities': 19313,\n",
       " 'tends': 12246,\n",
       " '##lift': 12215,\n",
       " 'て': 911,\n",
       " 'repulsed': 27986,\n",
       " 'Gretchen': 23886,\n",
       " 'Pirates': 11286,\n",
       " 'about': 1164,\n",
       " 'gene': 5565,\n",
       " '##versible': 24582,\n",
       " '##tron': 19138,\n",
       " 'Advocate': 20418,\n",
       " 'remixes': 20168,\n",
       " '##amia': 26817,\n",
       " '##oda': 16848,\n",
       " 'Telugu': 12321,\n",
       " 'Gamma': 23039,\n",
       " '##lithic': 26238,\n",
       " 'prevailed': 21691,\n",
       " 'sides': 3091,\n",
       " 'Massachusetts': 3559,\n",
       " 'discomfort': 19614,\n",
       " 'Buffy': 20495,\n",
       " 'essay': 10400,\n",
       " '##rouch': 22454,\n",
       " '##books': 16429,\n",
       " '1840s': 21000,\n",
       " '##liptic': 24021,\n",
       " 'Woodward': 20825,\n",
       " 'Management': 3973,\n",
       " 'LL': 12427,\n",
       " 'Stevenson': 14012,\n",
       " 'Navarre': 23411,\n",
       " 'positioned': 11059,\n",
       " 'Gallagher': 18497,\n",
       " 'prayers': 13865,\n",
       " 'finishing': 4416,\n",
       " '##play': 11044,\n",
       " 'Wire': 24781,\n",
       " 'feast': 11995,\n",
       " 'large': 1415,\n",
       " 'Moving': 13091,\n",
       " 'Medieval': 15902,\n",
       " 'astronomical': 20771,\n",
       " '##pper': 11292,\n",
       " 'exposition': 27634,\n",
       " 'armies': 9099,\n",
       " 'rocky': 10987,\n",
       " 'Hancock': 14305,\n",
       " '##shin': 18968,\n",
       " 'nickname': 8002,\n",
       " '##wen': 10781,\n",
       " 'tow': 18345,\n",
       " 'Hour': 12197,\n",
       " 'exaggerated': 18088,\n",
       " 'parish': 3363,\n",
       " 'robust': 17351,\n",
       " '##roduced': 27318,\n",
       " 'inward': 23120,\n",
       " 'revive': 19895,\n",
       " 'discover': 7290,\n",
       " 'filter': 12123,\n",
       " '##glass': 16802,\n",
       " 'writings': 7961,\n",
       " '##bol': 15792,\n",
       " 'Homestead': 25073,\n",
       " 'novelty': 25310,\n",
       " 'suggestion': 10219,\n",
       " 'म': 619,\n",
       " 'ₖ': 829,\n",
       " 'solely': 9308,\n",
       " 'wealth': 6968,\n",
       " 'acting': 3176,\n",
       " 'Duchy': 13019,\n",
       " '##ow': 4064,\n",
       " 'ニ': 956,\n",
       " '##hibit': 23034,\n",
       " '##₆': 28713,\n",
       " 'Queens': 9347,\n",
       " 'grasped': 16375,\n",
       " 'shortage': 16907,\n",
       " 'nutrition': 20121,\n",
       " '##ი': 28603,\n",
       " 'printed': 5757,\n",
       " 'streaks': 24177,\n",
       " 'reactions': 9535,\n",
       " 'aka': 10908,\n",
       " 'Clearly': 19260,\n",
       " '–': 782,\n",
       " 'Pole': 13172,\n",
       " 'eastward': 18953,\n",
       " 'administered': 8318,\n",
       " 'charts': 5896,\n",
       " 'Ahead': 21735,\n",
       " '##rissa': 26509,\n",
       " 'bicycle': 11639,\n",
       " '##otics': 24262,\n",
       " 'spell': 5814,\n",
       " 'Sherlock': 21897,\n",
       " 'actual': 4315,\n",
       " 'pinyin': 10307,\n",
       " 'ugly': 10126,\n",
       " 'sodium': 15059,\n",
       " '##brush': 20248,\n",
       " '##omi': 18882,\n",
       " '260': 13888,\n",
       " 'dad': 4153,\n",
       " 'Atlantis': 17793,\n",
       " 'Nationale': 28077,\n",
       " 'sorry': 2959,\n",
       " 'crew': 3039,\n",
       " 'Flash': 12346,\n",
       " 'Turtle': 26024,\n",
       " 'hunger': 10023,\n",
       " 'fault': 6088,\n",
       " 'Monthly': 17300,\n",
       " '##bbs': 20763,\n",
       " '##low': 6737,\n",
       " 'horses': 4697,\n",
       " 'appeared': 1691,\n",
       " '##š': 10222,\n",
       " 'located': 1388,\n",
       " 'Utrecht': 19789,\n",
       " 'spy': 10669,\n",
       " 'wetlands': 20432,\n",
       " 'yields': 17376,\n",
       " '##leen': 21180,\n",
       " 'Graf': 25125,\n",
       " 'likewise': 15869,\n",
       " '##wayne': 25920,\n",
       " 'wrong': 2488,\n",
       " 'Josiah': 27549,\n",
       " '##mulating': 24297,\n",
       " 'Having': 5823,\n",
       " 'Sioux': 17680,\n",
       " 'presents': 8218,\n",
       " 'Goose': 23914,\n",
       " 'sandwiches': 25326,\n",
       " '##dder': 19541,\n",
       " 'turbulent': 26510,\n",
       " 'Plants': 25880,\n",
       " 'ordeal': 27768,\n",
       " 'wide': 2043,\n",
       " 'voter': 16977,\n",
       " 'acquisitions': 23345,\n",
       " 'trimmed': 24509,\n",
       " 'plus': 4882,\n",
       " 'Elijah': 14581,\n",
       " 'ﬁ': 1094,\n",
       " 'wings': 4743,\n",
       " 'Dove': 20169,\n",
       " 'equal': 4463,\n",
       " 'envelope': 11427,\n",
       " 'bedroom': 4255,\n",
       " 'Riga': 19813,\n",
       " 'costa': 20658,\n",
       " 'neutrality': 25449,\n",
       " 'snap': 11152,\n",
       " '##nco': 23573,\n",
       " 'Greatest': 10971,\n",
       " 'Anders': 16357,\n",
       " 'Walker': 4575,\n",
       " 'plague': 13824,\n",
       " 'tense': 8901,\n",
       " 'mist': 12791,\n",
       " 'Hawks': 14126,\n",
       " '##mmy': 16211,\n",
       " '##biotics': 25523,\n",
       " 'resisted': 13672,\n",
       " 'presently': 16269,\n",
       " 'Roe': 26301,\n",
       " 'Reno': 19139,\n",
       " 'diameter': 6211,\n",
       " 'classics': 18046,\n",
       " 'adaptations': 18830,\n",
       " 'games': 1638,\n",
       " 'potentially': 9046,\n",
       " 'elevation': 6252,\n",
       " 'Airborne': 16607,\n",
       " 'Czech': 4833,\n",
       " 'ah': 18257,\n",
       " 'Wolf': 6499,\n",
       " 'Colonial': 10319,\n",
       " 'clearance': 16443,\n",
       " 'Everton': 19433,\n",
       " '##arma': 24275,\n",
       " 'despair': 15546,\n",
       " '205': 17342,\n",
       " 'Saturday': 4306,\n",
       " 'Yahoo': 23535,\n",
       " 'statesman': 25248,\n",
       " '##ison': 7614,\n",
       " 'Haynes': 24098,\n",
       " 'ψ': 441,\n",
       " 'seed': 6478,\n",
       " 'Roosevelt': 8189,\n",
       " '##tty': 13932,\n",
       " '##ire': 5817,\n",
       " 'republic': 13911,\n",
       " 'Lights': 13951,\n",
       " '##ruising': 24310,\n",
       " 'resolve': 10820,\n",
       " 'Am': 7277,\n",
       " 'need': 1444,\n",
       " 'Attendance': 6868,\n",
       " 'frightening': 18523,\n",
       " 'essence': 12661,\n",
       " 'demonic': 27324,\n",
       " 'Normandy': 13652,\n",
       " '1790': 13728,\n",
       " 'ш': 499,\n",
       " 'know': 1221,\n",
       " 'Isabel': 11726,\n",
       " 'System': 3910,\n",
       " '−': 851,\n",
       " 'independent': 2457,\n",
       " '##ere': 9014,\n",
       " '##bi': 5567,\n",
       " 'underwent': 9315,\n",
       " 'opposing': 10137,\n",
       " '110': 6745,\n",
       " 'tiger': 13778,\n",
       " 'believed': 2475,\n",
       " '##ac': 7409,\n",
       " 'onset': 15415,\n",
       " '্': 669,\n",
       " 'content': 3438,\n",
       " 'licking': 20238,\n",
       " 'bleed': 24752,\n",
       " 'č': 285,\n",
       " '##raphy': 18385,\n",
       " 'blacksmith': 26450,\n",
       " 'SA': 13411,\n",
       " 'priesthood': 20343,\n",
       " '##bation': 19632,\n",
       " 'faithful': 12969,\n",
       " 'Ronald': 8565,\n",
       " 'diagnosis': 12645,\n",
       " 'Gun': 11274,\n",
       " 'guitar': 2092,\n",
       " 'swamps': 26216,\n",
       " '##enter': 25195,\n",
       " 'mosques': 26054,\n",
       " '##ệ': 28651,\n",
       " 'Nebraska': 8097,\n",
       " '##gold': 26684,\n",
       " 'scared': 5528,\n",
       " 'Fleet': 6711,\n",
       " 'real': 1842,\n",
       " 'Wales': 2717,\n",
       " 'apparently': 4547,\n",
       " '±': 212,\n",
       " '##zo': 6112,\n",
       " 'aggregate': 9453,\n",
       " 'undefeated': 16467,\n",
       " 'Worcester': 12710,\n",
       " '##shima': 21957,\n",
       " 'massacre': 11584,\n",
       " 'goes': 2947,\n",
       " 'bodied': 25448,\n",
       " '##lect': 18465,\n",
       " 'college': 2134,\n",
       " 'coalition': 7453,\n",
       " 'butter': 13742,\n",
       " 'Augustine': 14579,\n",
       " 'Nadia': 15686,\n",
       " 'curiously': 18941,\n",
       " '1926': 4082,\n",
       " 'Dell': 18451,\n",
       " 'Al': 2586,\n",
       " 'vest': 21531,\n",
       " 'Member': 3844,\n",
       " 'untouched': 25135,\n",
       " '1831': 10897,\n",
       " 'rabbi': 20887,\n",
       " 'camp': 3227,\n",
       " 'surfaced': 16713,\n",
       " '##wai': 20264,\n",
       " 'feud': 15081,\n",
       " 'Winkler': 22382,\n",
       " '##ção': 18052,\n",
       " 'stole': 10566,\n",
       " 'handy': 25997,\n",
       " 'sales': 3813,\n",
       " 'Canyon': 10084,\n",
       " '##load': 9607,\n",
       " 'Howe': 13724,\n",
       " 'se': 14516,\n",
       " 'Prescott': 22721,\n",
       " 'lost': 1575,\n",
       " '133': 15118,\n",
       " '##obbing': 22967,\n",
       " 'Reich': 14994,\n",
       " 'prevailing': 21301,\n",
       " 'wildlife': 10501,\n",
       " 'fighters': 7705,\n",
       " 'campaign': 2322,\n",
       " 'Growth': 20227,\n",
       " 'Hutchinson': 18322,\n",
       " '06': 5037,\n",
       " 'Type': 6902,\n",
       " '##白': 28951,\n",
       " 'Nassau': 15272,\n",
       " 'Kiran': 24967,\n",
       " 'navigation': 11167,\n",
       " 'Spa': 23665,\n",
       " 'wary': 16970,\n",
       " '##tooth': 21590,\n",
       " 'mildly': 21461,\n",
       " 'Warm': 26252,\n",
       " 'fish': 3489,\n",
       " '425': 25933,\n",
       " 'allegedly': 9273,\n",
       " 'preliminary': 9889,\n",
       " '##ngle': 22170,\n",
       " 'yeast': 25693,\n",
       " 'trombone': 16537,\n",
       " 'resurrected': 26529,\n",
       " 'consumer': 8440,\n",
       " 'drastically': 23173,\n",
       " 'planet': 5015,\n",
       " 'centimeters': 19567,\n",
       " 'metres': 2759,\n",
       " '##UP': 18124,\n",
       " '##ع': 28490,\n",
       " 'Underwood': 25402,\n",
       " '[unused46]': 46,\n",
       " 'write': 3593,\n",
       " '##em': 5521,\n",
       " 'Intelligence': 7829,\n",
       " 'ː': 385,\n",
       " 'Hong': 3475,\n",
       " '##ously': 9537,\n",
       " '##nin': 10430,\n",
       " 'Help': 12056,\n",
       " 'storms': 14041,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e99f4187-19dd-4f2f-a794-e1ddc364acd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36326f33-8ed6-46d6-bbca-6e498b2f4f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28996"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30387f21-0675-4a35-9022-6dc52b0380e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Using a Transformer network is simple. But I think I can learn it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6661d-82ed-4d39-84eb-38109481dc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48c684a9-5f45-40e4-9cde-886bcba2151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple', '.', 'But', 'I', 'think', 'I', 'can', 'learn', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "583f84c1-e78d-44c6-9431-e2ae355b90f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014, 119, 1252, 146, 1341, 146, 1169, 3858, 1122, 119]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82717a-c7c1-4ccd-bcca-73e19c3d8125",
   "metadata": {},
   "source": [
    "##### Rather than doing it this way, you can directly convert sentence to tokens and then numeber (tensors) form as -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5099705c-698f-48e9-a650-274d2ef5bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 119, 1252, 146, 1341, 146, 1169, 3858, 1122, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0b4638f-f89d-4bfb-b72c-d85d724d41a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 4819, 1142, 1177, 1277, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokanizer(\"I hate this so much!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a8994-d69e-419d-953f-adb8e77f1445",
   "metadata": {},
   "source": [
    "#### Check the tensor(i.e. ids) from vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19421936-0c4f-48f3-8a54-3535b8286693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3014"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['simple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "017f455c-5fd5-4bc1-b87d-5fb3971125ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c5d160b-d977-472a-9b97-caec54f8dd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 102)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['[CLS]'], tokenizer.vocab['[SEP]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d1b2ba-31cb-49a6-b7bf-b99978f080a9",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53d28190-008e-4479-8ae5-79427915192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([ 7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7a45c-3819-4326-b92e-d0475a24763c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c0750fe-0752-489e-a29e-94802b9b3be8",
   "metadata": {},
   "source": [
    "# Handling Multiple Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82aecd43-1064-4321-b915-60593bfd007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dcd018e-f12a-4cd7-a0ed-83f38ac8d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d15c1366-993d-46c7-9115-a9ba2fe3a2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snandanw\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118c13f4cf39492fb1637d2e71e36fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snandanw\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\snandanw\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90655ac9e3ac435493cc2620aee13b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690268d108b24c158710579b1b34d3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\snandanw\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a4278e38c245e49f6ee0e27808b67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2413a3b1-98db-42f3-ae0b-9bc5e0e8df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9a3418d-8f70-4bed-8937-e3a1710474e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ids)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# This line will fail.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\llm\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:994\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    992\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 994\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1003\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32m~\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\llm\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:794\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 794\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarn_if_padding_and_no_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\llm\\lib\\site-packages\\transformers\\modeling_utils.py:4347\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m   4344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   4346\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[1;32m-> 4347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[0;32m   4348\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4349\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4352\u001b[0m     )\n\u001b[0;32m   4354\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[0;32m   4355\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d0fdb00-4154-496c-96e1-1def4f0c435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The problem is that we sent a single sequence to the model, whereas Transformers models expect multiple sentences by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9a541716-52f6-4d6c-8ec8-51d8f377ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try again adding few things\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5757e57-7a02-46d5-aaf5-c21dcfc40a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6049298-512e-4419-8381-cb8cd7075dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = model(input_ids)\n",
    "print(output)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9e2509a9-0d93-453a-a0d6-d9587909084f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Softmax(dim=SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8090425e-baf1-431f-a8d5-b1c53f2fddc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fcc94fa-4040-4b53-80c5-df2f5c74c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "##### Lets create multiple sequences manualy-\n",
    "print(tokenizer.pad_token_id)\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "002ca1d1-bc97-4be0-855a-9a52b764b94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[200, 200, 200]], [[200, 200]], [[200, 200, 200], [200, 200, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence1_ids, sequence2_ids, batched_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "abf6676a-fd44-476a-b265-5775c79c037f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88127577-5375-4fd4-83a5-f31c19138161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There’s something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we’ve got completely different values!\\nThis is because the key feature of Transformer models is attention layers that contextualize each token. \\nThese will take into account the padding tokens since they attend to all of the tokens of a sequence. \\nTo get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, \\nwe need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"There’s something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but we’ve got completely different values!\n",
    "This is because the key feature of Transformer models is attention layers that contextualize each token. \n",
    "These will take into account the padding tokens since they attend to all of the tokens of a sequence. \n",
    "To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, \n",
    "we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3780436d-7117-4d91-862d-b3c5a75eda05",
   "metadata": {},
   "source": [
    "#### Attention masks\n",
    "\n",
    "Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
    "So attention layer will only consider the tensors whose attention mask =1, Hence keeping the importance of the meachanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d9237f59-c137-4833-93e7-188e391b43f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now lets do this as -\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "# This below step is needed\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafa0fd-e56c-4e69-8110-5d2b2bbc7bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed12ac8b-9544-4f52-859d-4bc9dcd88b6a",
   "metadata": {},
   "source": [
    "#### We are Manually doing this conversion and then applying model. But in general we can directly use Tokenizer and Model as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a3aac73-b7d4-4278-98e1-9aa6a9daf9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6de6d1d-8dd1-4764-976e-d0fde0df6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73b179e7-eeb9-4664-bd3d-e721e81cc8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs) # No paddings added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f69e8773-8eb5-4cf7-b365-273bb35aa636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "13ed106b-0811-4ff6-8d13-abe839370000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2061, 2031, 1045, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "595e119d-81a0-458c-acd8-fe36f12e33bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 2061, 2031, 1045, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "584216cd-7fe5-4caa-af06-f1626773408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "### Returning type\n",
    "\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae1c8fbc-c5c5-4b72-9966-213814302f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab9dc4-bc18-4af0-97ad-4d2a14bf2a47",
   "metadata": {},
   "source": [
    "## Wrapping up: From tokenizer to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab43381d-d672-4096-adb9-edaf778ee164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token :- \n",
      " {'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [-3.6183,  3.9137]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(f\"Token :- \\n {tokens}\")\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "145d30f6-e13c-4d51-adeb-b3fb004ed1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I hated it!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9d931900-4c0c-4d68-87f6-20786d8fa836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123],\n",
       "        [ 4.2356, -3.4542]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e24ab-d727-4986-b6c8-6e56462547eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
